#!/usr/bin/env python3
"""
Email Task Manager Data Migration Agent
Handle Gmail data import/export, email archival, and user data lifecycle
"""

import os
import json
import sqlite3
import csv
import zipfile
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import shutil

class EmailTaskDataMigrator:
    """Specialized data migration agent for Email Task Manager project"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.backend_path = self.project_root / "backend"
        self.database_path = self.backend_path / "instance" / "email_task_manager.db"
        self.migrations_dir = self.backend_path / "migrations"
        self.exports_dir = self.project_root / "data_exports"
        
        self.migration_results = {
            'import_tools': [],
            'export_tools': [],
            'archival_scripts': [],
            'cleanup_utilities': [],
            'data_validation': []
        }
    
    def run_complete_migration_setup(self) -> Dict[str, Any]:
        """Set up complete data migration infrastructure"""
        print("ðŸ“¦ Setting up Email Task Manager Data Migration System...")
        
        # Core migration tools
        self._create_gmail_import_tools()
        self._create_data_export_utilities()
        self._create_archival_system()
        self._create_cleanup_utilities()
        self._create_validation_tools()
        
        # Migration scripts
        self._create_migration_scripts()
        self._create_backup_system()
        
        return self._generate_migration_report()
    
    def _create_gmail_import_tools(self):
        """Create Gmail data import utilities"""
        print("ðŸ“¥ Creating Gmail import tools...")
        
        # Create import utilities directory
        import_dir = self.backend_path / "utils" / "import_tools"
        import_dir.mkdir(parents=True, exist_ok=True)
        
        # Gmail import utility
        gmail_importer = """#!/usr/bin/env python3
\"\"\"
Gmail Data Import Utility
Generated by Data Migration Agent
\"\"\"

import json
import sqlite3
from datetime import datetime
from typing import Dict, List, Any
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from backend import create_app, db
from backend.models.user import User
from backend.models.email import Email
from backend.models.task import Task

class GmailDataImporter:
    def __init__(self):
        self.app = create_app()
        self.imported_count = {'emails': 0, 'tasks': 0}
    
    def import_gmail_export(self, export_file: str, user_id: int) -> Dict[str, Any]:
        \"\"\"Import Gmail takeout data\"\"\"
        with self.app.app_context():
            try:
                with open(export_file, 'r') as f:
                    gmail_data = json.load(f)
                
                user = User.query.get(user_id)
                if not user:
                    return {'error': 'User not found'}
                
                # Import emails
                for email_data in gmail_data.get('emails', []):
                    self._import_email(email_data, user_id)
                
                db.session.commit()
                return {'status': 'success', 'imported': self.imported_count}
                
            except Exception as e:
                db.session.rollback()
                return {'error': str(e)}
    
    def _import_email(self, email_data: Dict, user_id: int):
        \"\"\"Import individual email\"\"\"
        existing_email = Email.query.filter_by(
            gmail_id=email_data.get('id'),
            user_id=user_id
        ).first()
        
        if existing_email:
            return
        
        email = Email(
            user_id=user_id,
            gmail_id=email_data.get('id'),
            thread_id=email_data.get('threadId'),
            subject=email_data.get('subject', ''),
            sender=email_data.get('from', ''),
            sender_email=email_data.get('fromEmail', ''),
            body=email_data.get('body', ''),
            received_at=datetime.fromisoformat(email_data.get('date')) if email_data.get('date') else datetime.now(),
            processed=False
        )
        
        db.session.add(email)
        self.imported_count['emails'] += 1

def main():
    if len(sys.argv) != 3:
        print("Usage: python gmail_importer.py <export_file.json> <user_id>")
        sys.exit(1)
    
    export_file = sys.argv[1]
    user_id = int(sys.argv[2])
    
    importer = GmailDataImporter()
    result = importer.import_gmail_export(export_file, user_id)
    print(json.dumps(result, indent=2))

if __name__ == "__main__":
    main()
"""
        
        gmail_importer_file = import_dir / "gmail_importer.py"
        with open(gmail_importer_file, 'w') as f:
            f.write(gmail_importer)
        
        try:
            os.chmod(gmail_importer_file, 0o755)
        except:
            pass
        
        self.migration_results['import_tools'].append({
            'type': 'Gmail Import Utility',
            'file': str(gmail_importer_file),
            'features': 'Gmail takeout import, duplicate detection, batch processing',
            'usage': 'python gmail_importer.py export.json user_id'
        })
    
    def _create_data_export_utilities(self):
        """Create data export utilities"""
        print("ðŸ“¤ Creating data export utilities...")
        
        # Ensure exports directory exists
        self.exports_dir.mkdir(exist_ok=True)
        
        export_utility = """#!/usr/bin/env python3
\"\"\"
Data Export Utility
Generated by Data Migration Agent
\"\"\"

import json
import csv
import sqlite3
import zipfile
from datetime import datetime
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from backend import create_app, db
from backend.models.user import User
from backend.models.email import Email
from backend.models.task import Task

class DataExporter:
    def __init__(self):
        self.app = create_app()
    
    def export_user_data(self, user_id: int, format: str = 'json') -> str:
        \"\"\"Export all user data\"\"\"
        with self.app.app_context():
            user = User.query.get(user_id)
            if not user:
                raise ValueError('User not found')
            
            # Gather user data
            emails = Email.query.filter_by(user_id=user_id).all()
            tasks = Task.query.filter_by(user_id=user_id).all()
            
            export_data = {
                'user': user.to_dict(),
                'emails': [email.to_dict() for email in emails],
                'tasks': [task.to_dict() for task in tasks],
                'export_date': datetime.now().isoformat(),
                'total_emails': len(emails),
                'total_tasks': len(tasks)
            }
            
            # Create export file
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            export_dir = Path(__file__).parent.parent.parent / 'data_exports'
            export_dir.mkdir(exist_ok=True)
            
            if format == 'json':
                export_file = export_dir / f'user_{user_id}_export_{timestamp}.json'
                with open(export_file, 'w') as f:
                    json.dump(export_data, f, indent=2, default=str)
            
            elif format == 'csv':
                export_file = export_dir / f'user_{user_id}_export_{timestamp}.zip'
                with zipfile.ZipFile(export_file, 'w') as zf:
                    # Export emails to CSV
                    emails_csv = 'emails.csv'
                    with open(emails_csv, 'w', newline='') as csvfile:
                        if emails:
                            writer = csv.DictWriter(csvfile, fieldnames=emails[0].to_dict().keys())
                            writer.writeheader()
                            for email in emails:
                                writer.writerow(email.to_dict())
                    zf.write(emails_csv)
                    
                    # Export tasks to CSV
                    tasks_csv = 'tasks.csv'
                    with open(tasks_csv, 'w', newline='') as csvfile:
                        if tasks:
                            writer = csv.DictWriter(csvfile, fieldnames=tasks[0].to_dict().keys())
                            writer.writeheader()
                            for task in tasks:
                                writer.writerow(task.to_dict())
                    zf.write(tasks_csv)
                    
                    # Cleanup temp files
                    Path(emails_csv).unlink()
                    Path(tasks_csv).unlink()
            
            return str(export_file)

def main():
    if len(sys.argv) < 2:
        print("Usage: python data_exporter.py <user_id> [format]")
        print("Formats: json (default), csv")
        sys.exit(1)
    
    user_id = int(sys.argv[1])
    format = sys.argv[2] if len(sys.argv) > 2 else 'json'
    
    exporter = DataExporter()
    try:
        export_file = exporter.export_user_data(user_id, format)
        print(f"Data exported to: {export_file}")
    except Exception as e:
        print(f"Export failed: {e}")

if __name__ == "__main__":
    main()
"""
        
        export_file = self.backend_path / "utils" / "data_exporter.py"
        with open(export_file, 'w') as f:
            f.write(export_utility)
        
        try:
            os.chmod(export_file, 0o755)
        except:
            pass
        
        self.migration_results['export_tools'].append({
            'type': 'Data Export Utility',
            'file': str(export_file),
            'features': 'JSON/CSV export, user data packaging, GDPR compliance',
            'usage': 'python data_exporter.py user_id [json|csv]'
        })
    
    def _create_archival_system(self):
        """Create email archival system"""
        print("ðŸ—„ï¸ Creating archival system...")
        
        archival_script = """#!/usr/bin/env python3
\"\"\"
Email Archival System
Generated by Data Migration Agent
\"\"\"

import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from backend import create_app, db
from backend.models.email import Email
from backend.models.task import Task

class EmailArchiver:
    def __init__(self, archive_days: int = 90):
        self.app = create_app()
        self.archive_days = archive_days
        self.archived_count = 0
    
    def archive_old_emails(self, dry_run: bool = True) -> Dict[str, int]:
        \"\"\"Archive emails older than specified days\"\"\"
        with self.app.app_context():
            cutoff_date = datetime.now() - timedelta(days=self.archive_days)
            
            # Find old processed emails
            old_emails = Email.query.filter(
                Email.received_at < cutoff_date,
                Email.processed == True
            ).all()
            
            if dry_run:
                return {
                    'would_archive': len(old_emails),
                    'cutoff_date': cutoff_date.isoformat()
                }
            
            # Archive emails (mark as archived instead of deleting)
            for email in old_emails:
                email.archived = True
                self.archived_count += 1
            
            db.session.commit()
            
            return {
                'archived_count': self.archived_count,
                'cutoff_date': cutoff_date.isoformat()
            }
    
    def cleanup_completed_tasks(self, days: int = 30, dry_run: bool = True) -> Dict[str, int]:
        \"\"\"Clean up old completed tasks\"\"\"
        with self.app.app_context():
            cutoff_date = datetime.now() - timedelta(days=days)
            
            old_tasks = Task.query.filter(
                Task.completed == True,
                Task.completed_at < cutoff_date
            ).all()
            
            if dry_run:
                return {
                    'would_cleanup': len(old_tasks),
                    'cutoff_date': cutoff_date.isoformat()
                }
            
            for task in old_tasks:
                db.session.delete(task)
            
            db.session.commit()
            return {'cleaned_tasks': len(old_tasks)}

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Email Task Manager Archival')
    parser.add_argument('--archive-days', type=int, default=90)
    parser.add_argument('--cleanup-days', type=int, default=30)
    parser.add_argument('--dry-run', action='store_true')
    parser.add_argument('--execute', action='store_true')
    
    args = parser.parse_args()
    
    archiver = EmailArchiver(args.archive_days)
    
    # Archive old emails
    email_result = archiver.archive_old_emails(dry_run=not args.execute)
    print(f"Email archival: {email_result}")
    
    # Cleanup old tasks
    task_result = archiver.cleanup_completed_tasks(args.cleanup_days, dry_run=not args.execute)
    print(f"Task cleanup: {task_result}")

if __name__ == "__main__":
    main()
"""
        
        archival_file = self.backend_path / "utils" / "email_archiver.py"
        with open(archival_file, 'w') as f:
            f.write(archival_script)
        
        try:
            os.chmod(archival_file, 0o755)
        except:
            pass
        
        self.migration_results['archival_scripts'].append({
            'type': 'Email Archival System',
            'file': str(archival_file),
            'features': 'Automatic archival, configurable retention, dry-run mode',
            'usage': 'python email_archiver.py --archive-days 90 --execute'
        })
    
    def _create_cleanup_utilities(self):
        """Create database cleanup utilities"""
        print("ðŸ§¹ Creating cleanup utilities...")
        
        cleanup_script = """#!/usr/bin/env python3
\"\"\"
Database Cleanup Utility
Generated by Data Migration Agent
\"\"\"

import sqlite3
from datetime import datetime
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from backend import create_app, db

class DatabaseCleaner:
    def __init__(self):
        self.app = create_app()
    
    def vacuum_database(self) -> Dict[str, Any]:
        \"\"\"Optimize database by running VACUUM\"\"\"
        with self.app.app_context():
            try:
                db.engine.execute('VACUUM')
                return {'status': 'success', 'message': 'Database vacuumed successfully'}
            except Exception as e:
                return {'status': 'error', 'message': str(e)}
    
    def analyze_database_size(self) -> Dict[str, Any]:
        \"\"\"Analyze database size and table statistics\"\"\"
        with self.app.app_context():
            stats = {}
            
            # Get table counts
            tables = ['users', 'emails', 'tasks']
            for table in tables:
                try:
                    result = db.engine.execute(f'SELECT COUNT(*) FROM {table}')
                    count = result.fetchone()[0]
                    stats[f'{table}_count'] = count
                except:
                    stats[f'{table}_count'] = 0
            
            # Get database file size
            db_path = Path(__file__).parent.parent / 'instance' / 'email_task_manager.db'
            if db_path.exists():
                stats['database_size_mb'] = db_path.stat().st_size / (1024 * 1024)
            
            return stats
    
    def remove_orphaned_records(self, dry_run: bool = True) -> Dict[str, int]:
        \"\"\"Remove orphaned records\"\"\"
        with self.app.app_context():
            removed = {'tasks': 0, 'emails': 0}
            
            # Find orphaned tasks (tasks without valid email references)
            orphaned_tasks = db.engine.execute('''
                SELECT t.id FROM tasks t 
                LEFT JOIN emails e ON t.email_id = e.id 
                WHERE t.email_id IS NOT NULL AND e.id IS NULL
            ''')
            
            task_ids = [row[0] for row in orphaned_tasks]
            
            if not dry_run and task_ids:
                db.engine.execute('DELETE FROM tasks WHERE id IN ({})'.format(
                    ','.join(map(str, task_ids))
                ))
                removed['tasks'] = len(task_ids)
            else:
                removed['tasks'] = len(task_ids)
            
            if not dry_run:
                db.session.commit()
            
            return removed

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Database Cleanup Utility')
    parser.add_argument('--vacuum', action='store_true', help='Vacuum database')
    parser.add_argument('--analyze', action='store_true', help='Analyze database')
    parser.add_argument('--cleanup', action='store_true', help='Remove orphaned records')
    parser.add_argument('--execute', action='store_true', help='Execute cleanup (not dry run)')
    
    args = parser.parse_args()
    
    cleaner = DatabaseCleaner()
    
    if args.analyze:
        stats = cleaner.analyze_database_size()
        print("Database Statistics:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    if args.vacuum:
        result = cleaner.vacuum_database()
        print(f"Vacuum result: {result}")
    
    if args.cleanup:
        result = cleaner.remove_orphaned_records(dry_run=not args.execute)
        print(f"Cleanup result: {result}")

if __name__ == "__main__":
    main()
"""
        
        cleanup_file = self.backend_path / "utils" / "database_cleaner.py"
        with open(cleanup_file, 'w') as f:
            f.write(cleanup_script)
        
        try:
            os.chmod(cleanup_file, 0o755)
        except:
            pass
        
        self.migration_results['cleanup_utilities'].append({
            'type': 'Database Cleanup Utility',
            'file': str(cleanup_file),
            'features': 'VACUUM operation, orphaned record cleanup, size analysis',
            'usage': 'python database_cleaner.py --analyze --vacuum --cleanup --execute'
        })
    
    def _create_validation_tools(self):
        """Create data validation tools"""
        print("âœ… Creating validation tools...")
        
        validator_script = """#!/usr/bin/env python3
\"\"\"
Data Validation Utility
Generated by Data Migration Agent
\"\"\"

from datetime import datetime
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from backend import create_app, db
from backend.models.user import User
from backend.models.email import Email
from backend.models.task import Task

class DataValidator:
    def __init__(self):
        self.app = create_app()
        self.validation_errors = []
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        \"\"\"Validate database integrity\"\"\"
        with self.app.app_context():
            results = {
                'valid': True,
                'errors': [],
                'warnings': [],
                'statistics': {}
            }
            
            # Validate user data
            users = User.query.all()
            results['statistics']['total_users'] = len(users)
            
            for user in users:
                if not user.email:
                    results['errors'].append(f'User {user.id} missing email')
                    results['valid'] = False
            
            # Validate email data
            emails = Email.query.all()
            results['statistics']['total_emails'] = len(emails)
            
            for email in emails:
                if not email.gmail_id:
                    results['errors'].append(f'Email {email.id} missing gmail_id')
                    results['valid'] = False
                
                if email.user_id and not User.query.get(email.user_id):
                    results['errors'].append(f'Email {email.id} has invalid user_id')
                    results['valid'] = False
            
            # Validate task data
            tasks = Task.query.all()
            results['statistics']['total_tasks'] = len(tasks)
            
            for task in tasks:
                if task.user_id and not User.query.get(task.user_id):
                    results['errors'].append(f'Task {task.id} has invalid user_id')
                    results['valid'] = False
                
                if task.email_id and not Email.query.get(task.email_id):
                    results['warnings'].append(f'Task {task.id} references non-existent email')
            
            return results
    
    def validate_import_file(self, file_path: str) -> Dict[str, Any]:
        \"\"\"Validate import file format\"\"\"
        import json
        
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            
            required_fields = ['emails']
            missing_fields = [field for field in required_fields if field not in data]
            
            if missing_fields:
                return {'valid': False, 'errors': f'Missing fields: {missing_fields}'}
            
            # Validate email structure
            emails = data.get('emails', [])
            for i, email in enumerate(emails):
                if 'id' not in email:
                    return {'valid': False, 'errors': f'Email {i} missing id field'}
            
            return {
                'valid': True,
                'email_count': len(emails),
                'file_size_mb': Path(file_path).stat().st_size / (1024 * 1024)
            }
            
        except Exception as e:
            return {'valid': False, 'errors': str(e)}

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Data Validation Utility')
    parser.add_argument('--validate-db', action='store_true', help='Validate database integrity')
    parser.add_argument('--validate-file', type=str, help='Validate import file')
    
    args = parser.parse_args()
    
    validator = DataValidator()
    
    if args.validate_db:
        result = validator.validate_data_integrity()
        print("Database Validation Results:")
        print(f"Valid: {result['valid']}")
        print(f"Statistics: {result['statistics']}")
        if result['errors']:
            print(f"Errors: {result['errors']}")
        if result['warnings']:
            print(f"Warnings: {result['warnings']}")
    
    if args.validate_file:
        result = validator.validate_import_file(args.validate_file)
        print(f"File Validation: {result}")

if __name__ == "__main__":
    main()
"""
        
        validator_file = self.backend_path / "utils" / "data_validator.py"
        with open(validator_file, 'w') as f:
            f.write(validator_script)
        
        try:
            os.chmod(validator_file, 0o755)
        except:
            pass
        
        self.migration_results['data_validation'].append({
            'type': 'Data Validation Utility',
            'file': str(validator_file),
            'features': 'Integrity checks, import validation, error reporting',
            'usage': 'python data_validator.py --validate-db --validate-file export.json'
        })
    
    def _create_migration_scripts(self):
        """Create main migration orchestration scripts"""
        print("ðŸš€ Creating migration scripts...")
        
        # Create scripts directory
        scripts_dir = self.project_root / "scripts"
        scripts_dir.mkdir(exist_ok=True)
        
        migration_script = """#!/bin/bash
# Data Migration Script
# Generated by Data Migration Agent

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
BACKEND_DIR="$PROJECT_DIR/backend"

# Colors
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
RED='\\033[0;31m'
NC='\\033[0m'

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Import Gmail data
import_gmail_data() {
    local export_file=$1
    local user_id=$2
    
    if [[ ! -f "$export_file" ]]; then
        log_error "Export file not found: $export_file"
        return 1
    fi
    
    log_info "Importing Gmail data for user $user_id..."
    cd "$BACKEND_DIR"
    python utils/import_tools/gmail_importer.py "$export_file" "$user_id"
}

# Export user data
export_user_data() {
    local user_id=$1
    local format=${2:-json}
    
    log_info "Exporting data for user $user_id in $format format..."
    cd "$BACKEND_DIR"
    python utils/data_exporter.py "$user_id" "$format"
}

# Archive old data
archive_data() {
    local days=${1:-90}
    local execute=${2:-false}
    
    log_info "Archiving data older than $days days..."
    cd "$BACKEND_DIR"
    
    if [[ "$execute" == "true" ]]; then
        python utils/email_archiver.py --archive-days "$days" --execute
    else
        python utils/email_archiver.py --archive-days "$days" --dry-run
    fi
}

# Validate database
validate_data() {
    log_info "Validating database integrity..."
    cd "$BACKEND_DIR"
    python utils/data_validator.py --validate-db
}

# Show usage
show_usage() {
    echo "Usage: $0 [COMMAND] [OPTIONS]"
    echo ""
    echo "Commands:"
    echo "  import <file> <user_id>    Import Gmail export file"
    echo "  export <user_id> [format]  Export user data (json|csv)"
    echo "  archive [days] [execute]   Archive old data"
    echo "  validate                   Validate database"
    echo "  cleanup                    Clean up database"
}

# Main function
main() {
    case "${1:-help}" in
        "import")
            if [[ $# -lt 3 ]]; then
                log_error "Import requires export file and user ID"
                show_usage
                exit 1
            fi
            import_gmail_data "$2" "$3"
            ;;
        "export")
            if [[ $# -lt 2 ]]; then
                log_error "Export requires user ID"
                show_usage
                exit 1
            fi
            export_user_data "$2" "${3:-json}"
            ;;
        "archive")
            archive_data "${2:-90}" "${3:-false}"
            ;;
        "validate")
            validate_data
            ;;
        "cleanup")
            log_info "Running database cleanup..."
            cd "$BACKEND_DIR"
            python utils/database_cleaner.py --analyze --vacuum --cleanup --execute
            ;;
        "help"|*)
            show_usage
            ;;
    esac
}

main "$@"
"""
        
        migration_script_file = scripts_dir / "data_migration.sh"
        with open(migration_script_file, 'w') as f:
            f.write(migration_script)
        
        try:
            os.chmod(migration_script_file, 0o755)
        except:
            pass
        
        self.migration_results['import_tools'].append({
            'type': 'Migration Orchestration Script',
            'file': str(migration_script_file),
            'features': 'Command-line interface, batch operations, validation',
            'usage': './scripts/data_migration.sh import|export|archive|validate|cleanup'
        })
    
    def _create_backup_system(self):
        """Create automated backup system"""
        print("ðŸ’¾ Creating backup system...")
        
        backup_script = """#!/usr/bin/env python3
\"\"\"
Automated Backup System
Generated by Data Migration Agent
\"\"\"

import shutil
import sqlite3
from datetime import datetime
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

class BackupManager:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.backend_path = self.project_root / "backend"
        self.backup_dir = self.project_root / "backups"
        self.backup_dir.mkdir(exist_ok=True)
    
    def create_full_backup(self) -> str:
        \"\"\"Create full system backup\"\"\"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"full_backup_{timestamp}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir()
        
        # Backup database
        db_source = self.backend_path / "instance" / "email_task_manager.db"
        if db_source.exists():
            shutil.copy2(db_source, backup_path / "database.db")
        
        # Backup configuration files
        config_files = [".env", "requirements.txt"]
        for config_file in config_files:
            config_path = self.backend_path / config_file
            if config_path.exists():
                shutil.copy2(config_path, backup_path)
        
        # Create backup manifest
        manifest = {
            'backup_date': datetime.now().isoformat(),
            'backup_type': 'full',
            'files': [
                'database.db',
                *config_files
            ]
        }
        
        with open(backup_path / "manifest.json", 'w') as f:
            import json
            json.dump(manifest, f, indent=2)
        
        return str(backup_path)
    
    def restore_backup(self, backup_path: str) -> bool:
        \"\"\"Restore from backup\"\"\"
        backup_dir = Path(backup_path)
        if not backup_dir.exists():
            return False
        
        # Restore database
        backup_db = backup_dir / "database.db"
        target_db = self.backend_path / "instance" / "email_task_manager.db"
        
        if backup_db.exists():
            # Create backup of current database
            if target_db.exists():
                shutil.move(target_db, f"{target_db}.backup")
            
            shutil.copy2(backup_db, target_db)
        
        return True

def main():
    import argparse
    parser = argparse.ArgumentParser(description='Backup Manager')
    parser.add_argument('--backup', action='store_true', help='Create backup')
    parser.add_argument('--restore', type=str, help='Restore from backup path')
    
    args = parser.parse_args()
    
    manager = BackupManager()
    
    if args.backup:
        backup_path = manager.create_full_backup()
        print(f"Backup created: {backup_path}")
    
    if args.restore:
        success = manager.restore_backup(args.restore)
        print(f"Restore {'successful' if success else 'failed'}")

if __name__ == "__main__":
    main()
"""
        
        backup_file = self.backend_path / "utils" / "backup_manager.py"
        with open(backup_file, 'w') as f:
            f.write(backup_script)
        
        try:
            os.chmod(backup_file, 0o755)
        except:
            pass
        
        self.migration_results['archival_scripts'].append({
            'type': 'Backup Manager',
            'file': str(backup_file),
            'features': 'Full system backup, restore functionality, backup manifest',
            'usage': 'python backup_manager.py --backup --restore path'
        })
    
    def _generate_migration_report(self) -> Dict[str, Any]:
        """Generate comprehensive migration report"""
        
        total_tools = sum(len(tools) for tools in self.migration_results.values())
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'project': 'Email Task Manager',
            'migration_summary': {
                'total_tools': total_tools,
                'import_tools': len(self.migration_results['import_tools']),
                'export_tools': len(self.migration_results['export_tools']),
                'archival_scripts': len(self.migration_results['archival_scripts']),
                'cleanup_utilities': len(self.migration_results['cleanup_utilities']),
                'data_validation': len(self.migration_results['data_validation'])
            },
            'migration_components': self.migration_results,
            'key_features': [
                "ðŸ“¥ Gmail takeout data import with duplicate detection",
                "ðŸ“¤ User data export in JSON/CSV formats (GDPR compliant)",
                "ðŸ—„ï¸ Automated email archival with configurable retention",
                "ðŸ§¹ Database cleanup and optimization tools",
                "âœ… Data integrity validation and error reporting",
                "ðŸ’¾ Full system backup and restore capabilities",
                "ðŸš€ Command-line orchestration scripts"
            ],
            'usage_examples': [
                "./scripts/data_migration.sh import gmail_export.json 1",
                "./scripts/data_migration.sh export 1 json",
                "./scripts/data_migration.sh archive 90 true",
                "./scripts/data_migration.sh validate",
                "./scripts/data_migration.sh cleanup"
            ]
        }
        
        # Save report
        report_file = self.project_root / "data_migration_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nðŸ“¦ Data Migration Setup Complete!")
        print(f"Total Tools: {total_tools}")
        print(f"Report saved to: {report_file}")
        
        return report


def main():
    """Main execution function"""
    import sys
    
    project_root = sys.argv[1] if len(sys.argv) > 1 else os.getcwd()
    
    migrator = EmailTaskDataMigrator(project_root)
    report = migrator.run_complete_migration_setup()
    
    # Print summary
    print(f"\n{'='*60}")
    print(f"DATA MIGRATION SETUP SUMMARY")
    print(f"{'='*60}")
    print(f"Total Tools: {report['migration_summary']['total_tools']}")
    print(f"Import Tools: {report['migration_summary']['import_tools']}")
    print(f"Export Tools: {report['migration_summary']['export_tools']}")
    print(f"Archival Scripts: {report['migration_summary']['archival_scripts']}")
    print(f"Cleanup Utilities: {report['migration_summary']['cleanup_utilities']}")
    print(f"Validation Tools: {report['migration_summary']['data_validation']}")
    
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)